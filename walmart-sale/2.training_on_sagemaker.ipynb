{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2. GluonTS Training on Amazon SageMaker\n",
    "---\n",
    "\n",
    "본 모듈에서는 Amazon SageMaker API를 호출하여 모델 훈련을 수행합니다. 노트북 실행에는 약 10분 가량 소요되며, 핸즈온 실습 시에는 25분을 권장드립니다.\n",
    "\n",
    "Amazon SageMaker는 완전관리형 머신 러닝 서비스로 인프라 관리에 대해 걱정할 필요가 없으며, 딥러닝 프레임워크의 훈련/배포 컨테이너 이미지를 가져 와서\n",
    "여러분의 스크립트 코드를 쉽게 통합할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Training script\n",
    "---\n",
    "\n",
    "아래 코드 셀은 `src` 디렉토리에 SageMaker 훈련 스크립트인 `train.py`를 저장합니다.\n",
    "아래 스크립트가 이전 모듈의 코드와 대부분 일치하다는 점을 알 수 있습니다. 다시 말해, SageMaker 훈련 스크립트 파일은 기존 온프레미스에서 사용했던 Python 스크립트 파일과 크게 다르지 않으며, SageMaker 훈련 컨테이너에서 수행하기 위한 추가적인 환경 변수들만 설정하시면 됩니다.\n",
    "\n",
    "환경 변수 설정의 code snippet은 아래과 같습니다.\n",
    "\n",
    "```python\n",
    "# SageMaker Container environment\n",
    "parser.add_argument('--model_dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "parser.add_argument('--data_dir', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\n",
    "parser.add_argument('--num_gpus', type=int, default=os.environ['SM_NUM_GPUS'])\n",
    "parser.add_argument('--output_dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/train.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import gluonts \n",
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import pathlib\n",
    "from mxnet import gpu, cpu\n",
    "from mxnet.context import num_gpus\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gluonts.dataset.util import to_pandas\n",
    "from gluonts.mx.distribution import DistributionOutput, StudentTOutput, NegativeBinomialOutput, GaussianOutput\n",
    "from gluonts.model.simple_feedforward import SimpleFeedForwardEstimator\n",
    "from gluonts.model.deepar import DeepAREstimator\n",
    "from gluonts.mx.trainer import Trainer\n",
    "from gluonts.evaluation import Evaluator\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions, backtest_metrics\n",
    "from gluonts.model.predictor import Predictor\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.dataset.common import ListDataset\n",
    "\n",
    "def train(args):\n",
    "    \n",
    "    # Parse arguments\n",
    "    epochs = args.epochs\n",
    "    pred_length = args.pred_length\n",
    "    num_layers = args.num_layers\n",
    "    num_cells = args.num_cells\n",
    "    dropout_rate = args.dropout_rate\n",
    "    batch_size = args.batch_size\n",
    "    lr = args.lr\n",
    "    \n",
    "    model_dir = args.model_dir\n",
    "    data_dir = args.data_dir\n",
    "    num_gpus = args.num_gpus\n",
    "    output_dir = args.output_dir\n",
    "    \n",
    "    device = \"gpu\" if num_gpus > 0 else \"cpu\"\n",
    "    FREQ = 'D'    \n",
    "    \n",
    "    target_col = 'Weekly_Sales_sum'\n",
    "    related_cols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
    "       \n",
    "    # Get training data\n",
    "    target_train_df = pd.read_csv(os.path.join(data_dir, 'target_train.csv'), index_col=0, header=[0,1])\n",
    "    related_train_df = pd.read_csv(os.path.join(data_dir, 'related_train.csv'), index_col=0, header=[0,1])\n",
    "    store_df = pd.read_csv(os.path.join(data_dir, 'item.csv'), index_col=0)\n",
    "    \n",
    "    num_steps, num_series = target_train_df.shape\n",
    "    target = target_train_df.values\n",
    "\n",
    "    start_train_dt = target_train_df.index[0]\n",
    "    custom_ds_metadata = {'num_series': num_series, \n",
    "                          'num_steps': num_steps,\n",
    "                          'prediction_length': pred_length,\n",
    "                          'freq': FREQ,\n",
    "                          'start': [start_train_dt for _ in range(num_series)] \n",
    "                         }\n",
    "\n",
    "    # Prepare GlounTS Dataset\n",
    "    related_list = [related_train_df[c].values for c in related_cols]\n",
    "\n",
    "    train_lst = []\n",
    "    for i in range(0, num_series):\n",
    "        target_vec = target[:-pred_length, i]\n",
    "        related_vecs = [related[:-pred_length, i] for related in related_list]\n",
    "        item = store_df.loc[i+1]\n",
    "        dic = {FieldName.TARGET: target_vec, \n",
    "               FieldName.START: start_train_dt,\n",
    "               FieldName.FEAT_DYNAMIC_REAL: related_vecs,\n",
    "               FieldName.FEAT_STATIC_CAT: [item[0]],\n",
    "               FieldName.FEAT_STATIC_REAL: [item[1]]\n",
    "              } \n",
    "        train_lst.append(dic)\n",
    "\n",
    "    test_lst = []\n",
    "    for i in range(0, num_series):\n",
    "        target_vec = target[:, i]\n",
    "        related_vecs = [related[:, i] for related in related_list]\n",
    "        item = store_df.loc[i+1]    \n",
    "        dic = {FieldName.TARGET: target_vec, \n",
    "               FieldName.START: start_train_dt,\n",
    "               FieldName.FEAT_DYNAMIC_REAL: related_vecs,\n",
    "               FieldName.FEAT_STATIC_CAT: [item[0]],\n",
    "               FieldName.FEAT_STATIC_REAL: [item[1]]\n",
    "              } \n",
    "        test_lst.append(dic)\n",
    "\n",
    "    train_ds = ListDataset(train_lst, freq=FREQ)\n",
    "    test_ds = ListDataset(test_lst, freq=FREQ)   \n",
    "\n",
    "    # Define Estimator    \n",
    "    trainer = Trainer(\n",
    "        ctx=device,\n",
    "        epochs=epochs,\n",
    "        learning_rate=lr,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    deepar_estimator = DeepAREstimator(freq=FREQ, \n",
    "                                       prediction_length=pred_length,\n",
    "                                       use_feat_dynamic_real=True,\n",
    "                                       use_feat_static_cat=True,\n",
    "                                       use_feat_static_real=True,\n",
    "                                       cardinality=[3],\n",
    "                                       num_cells=30,\n",
    "                                       distr_output=StudentTOutput(),\n",
    "                                       trainer=trainer)\n",
    "    # Train the model\n",
    "    deepar_predictor = deepar_estimator.train(train_ds)\n",
    "    \n",
    "    # Evaluate trained model on test data\n",
    "    forecast_it, ts_it = make_evaluation_predictions(test_ds, deepar_predictor, num_samples=100)\n",
    "    forecasts = list(forecast_it)\n",
    "    tss = list(ts_it)\n",
    "    evaluator = Evaluator(quantiles=[0.1, 0.5, 0.9])\n",
    "    agg_metrics, item_metrics = evaluator(iter(tss), iter(forecasts), num_series=len(test_ds))\n",
    "\n",
    "    metrics = ['RMSE', 'MAPE', 'wQuantileLoss[0.1]', 'wQuantileLoss[0.5]', 'wQuantileLoss[0.9]', 'mean_wQuantileLoss']\n",
    "    metrics_dic = dict((key,value) for key, value in agg_metrics.items() if key in metrics)\n",
    "    print(json.dumps(metrics_dic, indent=2))\n",
    "\n",
    "    # Save the model\n",
    "    deepar_predictor.serialize(pathlib.Path(model_dir))\n",
    "    return deepar_predictor\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameter Setting\n",
    "    parser.add_argument('--epochs', type=int, default=30)\n",
    "    parser.add_argument('--pred_length', type=int, default=12)    \n",
    "    parser.add_argument('--num_layers', type=int, default=2)\n",
    "    parser.add_argument('--num_cells', type=int, default=30)\n",
    "    parser.add_argument('--dropout_rate', type=float, default=0.1)\n",
    "    parser.add_argument('--batch_size', type=float, default=32)\n",
    "    parser.add_argument('--lr', type=float, default=0.001) \n",
    "    \n",
    "    # SageMaker Container Environment\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--data_dir', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\n",
    "    parser.add_argument('--num_gpus', type=int, default=os.environ['SM_NUM_GPUS'])\n",
    "    parser.add_argument('--output_dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    return args    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parse_args()\n",
    "    train(args)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Training\n",
    "---\n",
    "\n",
    "스크립트가 준비되었다면 SageMaker 훈련을 수행하는 법은 매우 간단합니다. SageMaker Python SDK 활용 시, Estimator 인스턴스를 생성하고 해당 인스턴스의 `fit()` 메서드를 호출하는 것이 전부입니다. 좀 더 자세히 기술해 보면 아래와 같습니다.\n",
    "\n",
    "#### 1) Estimator 인스턴스 생성\n",
    "훈련 컨테이너에 필요한 설정들을 지정합니다. 본 핸즈온에서는 훈련 스크립트 파일이 포함된 경로인 소스 경로와(source_dir)와 훈련 스크립트 Python 파일만 엔트리포인트(entry_point)로 지정해 주면 됩니다.\n",
    "\n",
    "#### 2) `fit()` 메서드 호출\n",
    "`estimator.fit(YOUR_TRAINING_DATA_URI)` 메서드를 호출하면, 훈련에 필요한 인스턴스를 시작하고 컨테이너 환경을 시작합니다. 필수 인자값은 훈련 데이터가 존해자는 S3 경로(`s3://`)이며, 로컬 모드로 훈련 시에는 S3 경로와 로컬 경로(`file://`)를 모두 지정할 수 있습니다.\n",
    "\n",
    "인자값 중 wait은 디폴트 값으로 `wait=True`이며, 모든 훈련 작업이 완료될 때까지 코드 셀이 freezing됩니다. 만약 다른 코드 셀을 실행하거나, 다른 훈련 job을 시작하고 싶다면 `wait=False`로 설정하여 Asynchronous 모드로 변경하면 됩니다.\n",
    "\n",
    "**SageMaker 훈련이 끝나면 컨테이너 환경과 훈련 인스턴스는 자동으로 삭제됩니다.** 이 때, SageMaker는 자동으로 `SM_MODEL_DIR` 경로에 저장된 최종 모델 아티팩트를 `model.tar.gz`로 압축하여 훈련 컨테이너 환경에서 S3 bucket으로 저장합니다. 당연히, S3 bucket에 저장된 모델 아티팩트를 다운로드받아 로컬 상에서 곧바로 테스트할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.mxnet import MXNet\n",
    "\n",
    "boto_session = boto3.Session()\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto_session)\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker.Session().default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to Amazon S3\n",
    "\n",
    "Amazon SageMaker로 모델 훈련을 실행하기 위해, 데이터를 S3에 업로드합니다. 참고로, 로컬 모드에서 테스트 시에는 S3에 업로드할 필요 없이 로컬 상에서도 훈련이 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'timeseries-hol/walmart-sale/train'\n",
    "s3_bucket = boto3.Session().resource('s3').Bucket(bucket)\n",
    "\n",
    "s3_bucket.Object(os.path.join(prefix, 'target_train.csv')).upload_file('data/target_train.csv')\n",
    "s3_bucket.Object(os.path.join(prefix, 'related_train.csv')).upload_file('data/related_train.csv')\n",
    "s3_bucket.Object(os.path.join(prefix, 'item.csv')).upload_file('data/item.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Mode Training\n",
    "\n",
    "로컬 모드는 여러분이 작성한 훈련 및 배포 스크립트를 SageMaker에서 관리하는 클러스터에서 실행하기 전에 로컬 상(예: SageMaker 노트북 인스턴스, 개인 랩탑, 온프레미스)에서 여러분의 스크립트가 잘 동작하는지 빠르게 디버깅할 수 있는 방법입니다. 로컬 모드 훈련을 위해서는 docker-compose 또는 nvidia-docker-compose (GPU 인스턴스인 경우)가 필요하지만, SageMaker 노트북 인스턴스는 이미 docker가 설치되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = MXNet(entry_point='train.py',\n",
    "                    source_dir='src',\n",
    "                    role=role,\n",
    "                    instance_type='local',\n",
    "                    instance_count=1,\n",
    "                    framework_version='1.6.0',\n",
    "                    py_version='py3',\n",
    "                    hyperparameters = {'epochs': 2, \n",
    "                                       'lr': 0.001\n",
    "                                      }                       \n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating em4zmeopt2-algo-1-0ex5h ... \n",
      "Creating em4zmeopt2-algo-1-0ex5h ... done\n",
      "Attaching to em4zmeopt2-algo-1-0ex5h\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m 2021-04-07 06:55:20,385 sagemaker-training-toolkit INFO     Imported framework sagemaker_mxnet_container.training\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m 2021-04-07 06:55:20,388 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m 2021-04-07 06:55:20,402 sagemaker_mxnet_container.training INFO     MXNet training environment: {'SM_HOSTS': '[\"algo-1-0ex5h\"]', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_HPS': '{\"epochs\":2,\"lr\":0.001}', 'SM_USER_ENTRY_POINT': 'train.py', 'SM_FRAMEWORK_PARAMS': '{}', 'SM_RESOURCE_CONFIG': '{\"current_host\":\"algo-1-0ex5h\",\"hosts\":[\"algo-1-0ex5h\"]}', 'SM_INPUT_DATA_CONFIG': '{\"training\":{\"TrainingInputMode\":\"File\"}}', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_CHANNELS': '[\"training\"]', 'SM_CURRENT_HOST': 'algo-1-0ex5h', 'SM_MODULE_NAME': 'train', 'SM_LOG_LEVEL': '20', 'SM_FRAMEWORK_MODULE': 'sagemaker_mxnet_container.training:main', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_NUM_CPUS': '8', 'SM_NUM_GPUS': '0', 'SM_MODEL_DIR': '/opt/ml/model', 'SM_MODULE_DIR': 's3://sagemaker-us-east-1-143656149352/mxnet-training-2021-04-07-06-55-17-661/source/sourcedir.tar.gz', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1-0ex5h\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1-0ex5h\"],\"hyperparameters\":{\"epochs\":2,\"lr\":0.001},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"mxnet-training-2021-04-07-06-55-17-661\",\"log_level\":20,\"master_hostname\":\"algo-1-0ex5h\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-143656149352/mxnet-training-2021-04-07-06-55-17-661/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-0ex5h\",\"hosts\":[\"algo-1-0ex5h\"]},\"user_entry_point\":\"train.py\"}', 'SM_USER_ARGS': '[\"--epochs\",\"2\",\"--lr\",\"0.001\"]', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_CHANNEL_TRAINING': '/opt/ml/input/data/training', 'SM_HP_EPOCHS': '2', 'SM_HP_LR': '0.001'}\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m 2021-04-07 06:55:20,566 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m /usr/local/bin/python3.6 -m pip install -r requirements.txt\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m Collecting pandas==1.1.5\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m \u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/e2/00cacecafbab071c787019f00ad84ca3185952f6bb9bca9550ed83870d4d/pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5MB)\n",
      "\u001b[K     |████████████████████████████████| 9.5MB 18.4MB/s eta 0:00:01\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m \u001b[?25hCollecting gluonts==0.6.7\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m \u001b[?25l  Downloading https://files.pythonhosted.org/packages/2a/bf/66218eb5dcbd52d63e7ea3cb36bf6780cd639806fd9b7c7fc5c6074d4f10/gluonts-0.6.7-py3-none-any.whl (569kB)\n",
      "\u001b[K     |████████████████████████████████| 573kB 38.9MB/s eta 0:00:01\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m \u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas==1.1.5->-r requirements.txt (line 1)) (2021.1)\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/site-packages (from pandas==1.1.5->-r requirements.txt (line 1)) (1.19.1)\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/site-packages (from pandas==1.1.5->-r requirements.txt (line 1)) (2.8.0)\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m Collecting toolz~=0.10\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m \u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/f5/537e55f8ba664ff2a26f26913010fb0fcb98b6bbadc6158af888184fd0b7/toolz-0.11.1-py3-none-any.whl (55kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 12.1MB/s eta 0:00:01\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m \u001b[?25hRequirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.6/site-packages (from gluonts==0.6.7->-r requirements.txt (line 2)) (3.3.4)\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m Requirement already satisfied: tqdm~=4.23 in /usr/local/lib/python3.6/site-packages (from gluonts==0.6.7->-r requirements.txt (line 2)) (4.39.0)\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m Collecting ujson~=1.35\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m \u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/c4/79f3409bc710559015464e5f49b9879430d8f87498ecdc335899732e5377/ujson-1.35.tar.gz (192kB)\n",
      "\u001b[K     |████████████████████████████████| 194kB 44.7MB/s eta 0:00:01\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m \u001b[?25hCollecting pydantic<1.7,~=1.1\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m \u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/5f/855412ad12817ae87f1c77d3af2fc384eaed3adfb8f3994816d75483fa20/pydantic-1.6.1-cp36-cp36m-manylinux2014_x86_64.whl (8.7MB)\n",
      "\u001b[K     |████████████████████████████████| 8.7MB 38.1MB/s eta 0:00:01\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m \u001b[?25hCollecting holidays>=0.9\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m \u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/46/a471d6594325aeb5dc2a591d38eb5ae8b0703dda3bc8d9f656ec1ab00a07/holidays-0.11.1-py3-none-any.whl (133kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 41.8MB/s eta 0:00:01\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m \u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas==1.1.5->-r requirements.txt (line 1)) (1.15.0)\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/site-packages (from matplotlib~=3.0->gluonts==0.6.7->-r requirements.txt (line 2)) (8.1.1)\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/site-packages (from matplotlib~=3.0->gluonts==0.6.7->-r requirements.txt (line 2)) (0.10.0)\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/site-packages (from matplotlib~=3.0->gluonts==0.6.7->-r requirements.txt (line 2)) (1.3.1)\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/site-packages (from matplotlib~=3.0->gluonts==0.6.7->-r requirements.txt (line 2)) (2.4.7)\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m Collecting dataclasses>=0.6; python_version < \"3.7\"\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m   Downloading https://files.pythonhosted.org/packages/fe/ca/75fac5856ab5cfa51bbbcefa250182e50441074fdc3f803f6e76451fab43/dataclasses-0.8-py3-none-any.whl\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m Collecting convertdate>=2.3.0\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m \u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/fa/b5f5d8b3a328c930a190540231fe79e854a416df62c57329630823f3941e/convertdate-2.3.2-py3-none-any.whl (47kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 11.4MB/s eta 0:00:01\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m \u001b[?25hCollecting hijri-converter\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m   Downloading https://files.pythonhosted.org/packages/3e/75/e6da96d4ea768c8e6fa9676cffce80e457b66c3beb5711189959582870d6/hijri_converter-2.1.1-py3-none-any.whl\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m Collecting korean-lunar-calendar\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m   Downloading https://files.pythonhosted.org/packages/15/41/aa426a4a9141afd8e7f5c8312bb59d5693274f3f7b34e73bdce4ee48b4c1/korean_lunar_calendar-0.2.1-py3-none-any.whl\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m Collecting pymeeus<=1,>=0.3.13\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m \u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/ff/0f0a0becf088281c6bc6c75b7d7c03a2481d486ef6cc7c8899bbcab0a88d/PyMeeus-0.5.11.tar.gz (5.4MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m \r",
      "\u001b[K     |                                | 10kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |▏                               | 20kB 33.8MB/s eta 0:00:01\r",
      "\u001b[K     |▏                               | 30kB 39.1MB/s eta 0:00:01\r",
      "\u001b[K     |▎                               | 40kB 36.1MB/s eta 0:00:01\r",
      "\u001b[K     |▎                               | 51kB 36.7MB/s eta 0:00:01\r",
      "\u001b[K     |▍                               | 61kB 36.5MB/s eta 0:00:01\r",
      "\u001b[K     |▍                               | 71kB 34.1MB/s eta 0:00:01\r",
      "\u001b[K     |▌                               | 81kB 35.9MB/s eta 0:00:01\r",
      "\u001b[K     |▌                               | 92kB 36.8MB/s eta 0:00:01\r",
      "\u001b[K     |▋                               | 102kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |▊                               | 112kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |▊                               | 122kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |▉                               | 133kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |▉                               | 143kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 153kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 163kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 174kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 184kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█▏                              | 194kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█▏                              | 204kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█▎                              | 215kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█▍                              | 225kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█▍                              | 235kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█▌                              | 245kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█▌                              | 256kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█▋                              | 266kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█▋                              | 276kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█▊                              | 286kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█▊                              | 296kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█▉                              | 307kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 317kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 327kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 337kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 348kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██▏                             | 358kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██▏                             | 368kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██▎                             | 378kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██▎                             | 389kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██▍                             | 399kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██▍                             | 409kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██▌                             | 419kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██▋                             | 430kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██▋                             | 440kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██▊                             | 450kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██▊                             | 460kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██▉                             | 471kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██▉                             | 481kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 491kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 501kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 512kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▏                            | 522kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▏                            | 532kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▎                            | 542kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▎                            | 552kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▍                            | 563kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▍                            | 573kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▌                            | 583kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▌                            | 593kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▋                            | 604kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▋                            | 614kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▊                            | 624kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 634kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 645kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 655kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 665kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 675kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 686kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████▏                           | 696kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████▏                           | 706kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████▎                           | 716kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████▍                           | 727kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████▍                           | 737kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████▌                           | 747kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████▌                           | 757kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████▋                           | 768kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████▋                           | 778kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 788kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 798kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████▉                           | 808kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████▉                           | 819kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 829kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 839kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 849kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 860kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 870kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████▎                          | 880kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████▎                          | 890kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████▍                          | 901kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████▍                          | 911kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 921kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 931kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████▋                          | 942kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████▊                          | 952kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████▊                          | 962kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████▉                          | 972kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████▉                          | 983kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 993kB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 1.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 1.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 1.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████▏                         | 1.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 1.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 1.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████▍                         | 1.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████▍                         | 1.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████▌                         | 1.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████▌                         | 1.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████▋                         | 1.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████▋                         | 1.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 1.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 1.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████▉                         | 1.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 1.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 1.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 1.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 1.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▏                        | 1.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▏                        | 1.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 1.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 1.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▍                        | 1.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▌                        | 1.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▌                        | 1.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 1.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 1.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▊                        | 1.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▊                        | 1.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 1.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 1.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 1.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 1.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 1.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▏                       | 1.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▏                       | 1.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▎                       | 1.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▎                       | 1.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▍                       | 1.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▍                       | 1.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▌                       | 1.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▌                       | 1.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▋                       | 1.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▊                       | 1.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▊                       | 1.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▉                       | 1.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▉                       | 1.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 1.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 1.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 1.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 1.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▏                      | 1.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▏                      | 1.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▎                      | 1.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▍                      | 1.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▍                      | 1.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▌                      | 1.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▌                      | 1.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▋                      | 1.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▋                      | 1.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▊                      | 1.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▊                      | 1.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▉                      | 1.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▉                      | 1.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 1.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 1.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 1.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▏                     | 1.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▏                     | 1.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 1.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 1.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▍                     | 1.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▍                     | 1.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▌                     | 1.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▋                     | 1.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▋                     | 1.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▊                     | 1.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▊                     | 1.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▉                     | 1.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▉                     | 1.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 1.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 1.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 1.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 1.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▏                    | 1.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▎                    | 1.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▎                    | 1.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▍                    | 1.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▍                    | 1.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▌                    | 1.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▌                    | 1.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▋                    | 1.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▋                    | 2.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▊                    | 2.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▉                    | 2.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▉                    | 2.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 2.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 2.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 2.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 2.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▏                   | 2.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▏                   | 2.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▎                   | 2.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▎                   | 2.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▍                   | 2.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 2.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 2.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▋                   | 2.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▋                   | 2.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▊                   | 2.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▊                   | 2.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▉                   | 2.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▉                   | 2.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 2.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 2.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 2.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▏                  | 2.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▏                  | 2.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▎                  | 2.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▎                  | 2.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▍                  | 2.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▍                  | 2.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 2.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 2.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▋                  | 2.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 2.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 2.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▉                  | 2.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▉                  | 2.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 2.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 2.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 2.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 2.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▏                 | 2.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▏                 | 2.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▎                 | 2.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▍                 | 2.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▍                 | 2.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▌                 | 2.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▌                 | 2.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▋                 | 2.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▋                 | 2.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▊                 | 2.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▊                 | 2.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 2.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 2.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 2.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 2.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 2.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 2.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 2.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▎                | 2.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▎                | 2.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▍                | 2.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▍                | 2.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▌                | 2.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▋                | 2.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▋                | 2.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▊                | 2.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▊                | 2.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▉                | 2.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▉                | 2.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 2.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 2.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 2.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▏               | 2.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▏               | 2.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▎               | 2.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▎               | 2.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▍               | 2.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▍               | 2.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▌               | 2.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▌               | 2.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▋               | 2.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▋               | 2.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 2.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▉               | 2.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▉               | 2.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 2.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 2.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 2.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 2.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▏              | 2.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▏              | 2.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▎              | 2.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▍              | 2.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▍              | 2.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▌              | 2.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▌              | 2.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▋              | 2.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▋              | 3.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▊              | 3.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▊              | 3.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▉              | 3.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▉              | 3.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 3.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 3.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 3.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▏             | 3.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▏             | 3.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 3.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 3.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▍             | 3.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▍             | 3.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▌             | 3.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 3.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 3.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▊             | 3.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▊             | 3.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▉             | 3.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▉             | 3.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 3.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 3.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 3.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 3.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▏            | 3.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 3.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 3.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▍            | 3.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▍            | 3.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▌            | 3.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▌            | 3.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▋            | 3.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▋            | 3.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▊            | 3.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▊            | 3.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▉            | 3.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 3.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 3.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 3.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 3.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 3.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 3.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▎           | 3.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▎           | 3.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 3.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▌           | 3.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▌           | 3.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▋           | 3.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▋           | 3.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▊           | 3.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▊           | 3.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 3.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 3.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 3.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 3.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 3.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▏          | 3.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▏          | 3.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 3.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 3.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▍          | 3.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▍          | 3.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▌          | 3.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▌          | 3.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 3.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▊          | 3.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▊          | 3.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▉          | 3.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▉          | 3.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 3.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 3.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 3.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 3.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 3.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 3.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 3.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▍         | 3.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▍         | 3.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▌         | 3.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▌         | 3.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▋         | 3.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▋         | 3.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 3.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 3.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▉         | 3.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 3.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 3.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 3.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 3.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▏        | 3.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▏        | 3.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▎        | 3.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▎        | 3.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 3.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 3.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▌        | 3.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 4.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 4.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▊        | 4.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▊        | 4.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▉        | 4.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▉        | 4.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 4.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 4.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 4.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 4.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▏       | 4.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 4.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 4.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▍       | 4.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▍       | 4.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▌       | 4.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▌       | 4.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▋       | 4.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▋       | 4.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▊       | 4.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 4.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 4.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 4.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 4.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 4.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 4.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 4.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 4.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▎      | 4.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▎      | 4.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▍      | 4.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 4.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 4.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▋      | 4.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▋      | 4.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▊      | 4.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▊      | 4.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▉      | 4.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▉      | 4.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 4.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 4.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 4.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 4.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 4.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▎     | 4.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▎     | 4.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▍     | 4.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▍     | 4.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 4.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 4.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▋     | 4.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▊     | 4.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▊     | 4.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 4.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 4.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 4.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 4.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 4.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 4.5MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▏    | 4.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▎    | 4.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▎    | 4.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▍    | 4.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▍    | 4.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 4.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 4.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▋    | 4.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▋    | 4.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 4.6MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 4.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 4.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 4.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 4.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 4.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 4.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▏   | 4.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▏   | 4.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▎   | 4.7MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▎   | 4.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▍   | 4.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▍   | 4.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▌   | 4.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▋   | 4.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▋   | 4.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▊   | 4.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▊   | 4.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 4.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 4.8MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 4.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 4.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 4.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 4.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 4.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▎  | 4.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▎  | 4.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▍  | 4.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▍  | 4.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▌  | 4.9MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▌  | 5.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▋  | 5.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▋  | 5.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 5.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▉  | 5.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▉  | 5.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 5.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 5.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 5.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 5.0MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▏ | 5.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▏ | 5.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▎ | 5.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▍ | 5.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▍ | 5.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▌ | 5.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▌ | 5.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 5.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 5.1MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▊ | 5.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▊ | 5.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▉ | 5.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▉ | 5.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 5.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 5.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 5.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 5.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 5.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▎| 5.2MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▎| 5.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▍| 5.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▍| 5.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 5.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 5.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 5.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▊| 5.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▊| 5.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 5.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 5.3MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 5.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 5.4MB 35.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 5.4MB 35.8MB/s \r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m \u001b[?25hInstalling collected packages: pandas, toolz, ujson, dataclasses, pydantic, pymeeus, convertdate, hijri-converter, korean-lunar-calendar, holidays, gluonts\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m   Found existing installation: pandas 0.25.1\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m     Uninstalling pandas-0.25.1:\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m       Successfully uninstalled pandas-0.25.1\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m     Running setup.py install for ujson ... \u001b[?25ldone\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m \u001b[?25h    Running setup.py install for pymeeus ... \u001b[?25ldone\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m \u001b[?25hSuccessfully installed convertdate-2.3.2 dataclasses-0.8 gluonts-0.6.7 hijri-converter-2.1.1 holidays-0.11.1 korean-lunar-calendar-0.2.1 pandas-1.1.5 pydantic-1.6.1 pymeeus-0.5.11 toolz-0.11.1 ujson-1.35\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m \u001b[33mWARNING: You are using pip version 19.3.1; however, version 21.0.1 is available.\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m 2021-04-07 06:55:32,431 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m 2021-04-07 06:55:32,447 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m 2021-04-07 06:55:32,463 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m 2021-04-07 06:55:32,477 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m \n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m Training Env:\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m \n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m {\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m         \"training\": \"/opt/ml/input/data/training\"\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m     },\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m     \"current_host\": \"algo-1-0ex5h\",\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m     \"framework_module\": \"sagemaker_mxnet_container.training:main\",\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m     \"hosts\": [\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m         \"algo-1-0ex5h\"\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m     ],\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m         \"epochs\": 2,\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m         \"lr\": 0.001\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m     },\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m         \"training\": {\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m         }\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m     },\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m     \"job_name\": \"mxnet-training-2021-04-07-06-55-17-661\",\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m     \"master_hostname\": \"algo-1-0ex5h\",\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m     \"module_dir\": \"s3://sagemaker-us-east-1-143656149352/mxnet-training-2021-04-07-06-55-17-661/source/sourcedir.tar.gz\",\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m     \"module_name\": \"train\",\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m     \"num_cpus\": 8,\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m     \"num_gpus\": 0,\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m         \"current_host\": \"algo-1-0ex5h\",\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m         \"hosts\": [\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m             \"algo-1-0ex5h\"\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m         ]\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m     },\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m     \"user_entry_point\": \"train.py\"\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m }\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m \n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m Environment variables:\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m \n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m SM_HOSTS=[\"algo-1-0ex5h\"]\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m SM_HPS={\"epochs\":2,\"lr\":0.001}\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m SM_USER_ENTRY_POINT=train.py\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-0ex5h\",\"hosts\":[\"algo-1-0ex5h\"]}\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m SM_INPUT_DATA_CONFIG={\"training\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m SM_CHANNELS=[\"training\"]\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m SM_CURRENT_HOST=algo-1-0ex5h\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m SM_MODULE_NAME=train\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_mxnet_container.training:main\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m SM_NUM_CPUS=8\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m SM_NUM_GPUS=0\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m SM_MODULE_DIR=s3://sagemaker-us-east-1-143656149352/mxnet-training-2021-04-07-06-55-17-661/source/sourcedir.tar.gz\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1-0ex5h\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1-0ex5h\"],\"hyperparameters\":{\"epochs\":2,\"lr\":0.001},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"mxnet-training-2021-04-07-06-55-17-661\",\"log_level\":20,\"master_hostname\":\"algo-1-0ex5h\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-143656149352/mxnet-training-2021-04-07-06-55-17-661/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-0ex5h\",\"hosts\":[\"algo-1-0ex5h\"]},\"user_entry_point\":\"train.py\"}\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m SM_USER_ARGS=[\"--epochs\",\"2\",\"--lr\",\"0.001\"]\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m SM_HP_EPOCHS=2\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m SM_HP_LR=0.001\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m PYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python36.zip:/usr/local/lib/python3.6:/usr/local/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/site-packages\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m \n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m \n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m /usr/local/bin/python3.6 train.py --epochs 2 --lr 0.001\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m \n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m \n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m learning rate from ``lr_scheduler`` has been overwritten by ``learning_rate`` in optimizer.\n",
      "  0% 0/50 [00:00<?, ?it/s]INFO:gluonts.trainer:Number of parameters in DeepARTrainingNetwork: 16419\n",
      "100% 50/50 [00:02<00:00, 20.47it/s, epoch=1/2, avg_epoch_loss=2.47]\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m INFO:gluonts.trainer:Epoch[0] Elapsed time 2.444 seconds\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m INFO:gluonts.trainer:Epoch[0] Evaluation metric 'epoch_loss'=2.467834\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m INFO:gluonts.trainer:Epoch[1] Learning rate is 0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% 50/50 [00:02<00:00, 24.77it/s, epoch=2/2, avg_epoch_loss=1.52]\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m INFO:gluonts.trainer:Epoch[1] Elapsed time 2.019 seconds\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m INFO:gluonts.trainer:Epoch[1] Evaluation metric 'epoch_loss'=1.518441\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m INFO:root:Computing averaged parameters.\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m INFO:root:Loading averaged parameters.\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m INFO:gluonts.trainer:End model training\n",
      "Running evaluation: 100% 45/45 [00:00<00:00, 1256.26it/s]\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m {\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m   \"MAPE\": 0.06744109699209787,\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m   \"RMSE\": 0.8614203206290676,\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m   \"wQuantileLoss[0.1]\": 0.02350694884589814,\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m   \"wQuantileLoss[0.5]\": 0.0637844221455642,\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m   \"wQuantileLoss[0.9]\": 0.033332416264957775,\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m   \"mean_wQuantileLoss\": 0.040207929085473375\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m }\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m WARNING:root:Serializing RepresentableBlockPredictor instances does not save the prediction network structure in a backwards-compatible manner. Be careful not to use this method in production.\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h |\u001b[0m 2021-04-07 06:55:40,870 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\u001b[36mem4zmeopt2-algo-1-0ex5h exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "s3_input = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}'.format(bucket, prefix))\n",
    "estimator.fit(s3_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재 실행 중인 도커 컨테이너가 없는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\r\n"
     ]
    }
   ],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Hosted Training\n",
    "\n",
    "훈련 코드가 로컬에서 잘 작동하므로, 이제 SageMaker에서 관리하는 훈련 인스턴스를 사용하여 훈련을 수행하겠습니다. 로컬 모드 훈련과 달리 호스팅 훈련은\n",
    "노트북 인스턴스 대신에 SageMaker에서 관리하는 별도의 클러스터에서 수행합니다. 본 핸즈온의 데이터셋 사이즈가 작기 때문에 체감이 되지 않겠지만, 대규모 데이터 및 복잡한 모델에 대한 분산 훈련은 SageMaker 호스팅 훈련 방법을 사용하는 것을 권장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = MXNet(entry_point='train.py',\n",
    "                    source_dir='src',\n",
    "                    role=role,\n",
    "                    instance_type='ml.c5.xlarge',\n",
    "                    instance_count=1,\n",
    "                    framework_version='1.6.0',\n",
    "                    py_version='py3',\n",
    "                    hyperparameters = {'epochs': 30, \n",
    "                                       'lr': 0.001,\n",
    "                                      }                       \n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-07 06:55:55 Starting - Starting the training job...\n",
      "2021-04-07 06:56:19 Starting - Launching requested ML instancesProfilerReport-1617778554: InProgress\n",
      ".........\n",
      "2021-04-07 06:57:39 Starting - Preparing the instances for training...\n",
      "2021-04-07 06:58:19 Downloading - Downloading input data...\n",
      "2021-04-07 06:58:54 Training - Training image download completed. Training in progress..\u001b[34m2021-04-07 06:58:54,911 sagemaker-training-toolkit INFO     Imported framework sagemaker_mxnet_container.training\u001b[0m\n",
      "\u001b[34m2021-04-07 06:58:54,913 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-04-07 06:58:54,923 sagemaker_mxnet_container.training INFO     MXNet training environment: {'SM_HOSTS': '[\"algo-1\"]', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_HPS': '{\"epochs\":30,\"lr\":0.001}', 'SM_USER_ENTRY_POINT': 'train.py', 'SM_FRAMEWORK_PARAMS': '{}', 'SM_RESOURCE_CONFIG': '{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}', 'SM_INPUT_DATA_CONFIG': '{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_CHANNELS': '[\"training\"]', 'SM_CURRENT_HOST': 'algo-1', 'SM_MODULE_NAME': 'train', 'SM_LOG_LEVEL': '20', 'SM_FRAMEWORK_MODULE': 'sagemaker_mxnet_container.training:main', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_NUM_CPUS': '4', 'SM_NUM_GPUS': '0', 'SM_MODEL_DIR': '/opt/ml/model', 'SM_MODULE_DIR': 's3://sagemaker-us-east-1-143656149352/mxnet-training-2021-04-07-06-55-54-757/source/sourcedir.tar.gz', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":30,\"lr\":0.001},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"mxnet-training-2021-04-07-06-55-54-757\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-143656149352/mxnet-training-2021-04-07-06-55-54-757/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}', 'SM_USER_ARGS': '[\"--epochs\",\"30\",\"--lr\",\"0.001\"]', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_CHANNEL_TRAINING': '/opt/ml/input/data/training', 'SM_HP_LR': '0.001', 'SM_HP_EPOCHS': '30'}\u001b[0m\n",
      "\u001b[34m2021-04-07 06:59:10,800 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting pandas==1.1.5\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/e2/00cacecafbab071c787019f00ad84ca3185952f6bb9bca9550ed83870d4d/pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5MB)\u001b[0m\n",
      "\u001b[34mCollecting gluonts==0.6.7\n",
      "  Downloading https://files.pythonhosted.org/packages/2a/bf/66218eb5dcbd52d63e7ea3cb36bf6780cd639806fd9b7c7fc5c6074d4f10/gluonts-0.6.7-py3-none-any.whl (569kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/site-packages (from pandas==1.1.5->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas==1.1.5->-r requirements.txt (line 1)) (2021.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/site-packages (from pandas==1.1.5->-r requirements.txt (line 1)) (2.8.0)\u001b[0m\n",
      "\u001b[34mCollecting holidays>=0.9\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/46/a471d6594325aeb5dc2a591d38eb5ae8b0703dda3bc8d9f656ec1ab00a07/holidays-0.11.1-py3-none-any.whl (133kB)\u001b[0m\n",
      "\u001b[34mCollecting toolz~=0.10\n",
      "  Downloading https://files.pythonhosted.org/packages/12/f5/537e55f8ba664ff2a26f26913010fb0fcb98b6bbadc6158af888184fd0b7/toolz-0.11.1-py3-none-any.whl (55kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm~=4.23 in /usr/local/lib/python3.6/site-packages (from gluonts==0.6.7->-r requirements.txt (line 2)) (4.39.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.6/site-packages (from gluonts==0.6.7->-r requirements.txt (line 2)) (3.3.4)\u001b[0m\n",
      "\u001b[34mCollecting ujson~=1.35\n",
      "  Downloading https://files.pythonhosted.org/packages/16/c4/79f3409bc710559015464e5f49b9879430d8f87498ecdc335899732e5377/ujson-1.35.tar.gz (192kB)\u001b[0m\n",
      "\u001b[34mCollecting pydantic<1.7,~=1.1\n",
      "  Downloading https://files.pythonhosted.org/packages/aa/5f/855412ad12817ae87f1c77d3af2fc384eaed3adfb8f3994816d75483fa20/pydantic-1.6.1-cp36-cp36m-manylinux2014_x86_64.whl (8.7MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas==1.1.5->-r requirements.txt (line 1)) (1.15.0)\u001b[0m\n",
      "\u001b[34mCollecting korean-lunar-calendar\n",
      "  Downloading https://files.pythonhosted.org/packages/15/41/aa426a4a9141afd8e7f5c8312bb59d5693274f3f7b34e73bdce4ee48b4c1/korean_lunar_calendar-0.2.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mCollecting convertdate>=2.3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/1b/fa/b5f5d8b3a328c930a190540231fe79e854a416df62c57329630823f3941e/convertdate-2.3.2-py3-none-any.whl (47kB)\u001b[0m\n",
      "\u001b[34mCollecting hijri-converter\n",
      "  Downloading https://files.pythonhosted.org/packages/3e/75/e6da96d4ea768c8e6fa9676cffce80e457b66c3beb5711189959582870d6/hijri_converter-2.1.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/site-packages (from matplotlib~=3.0->gluonts==0.6.7->-r requirements.txt (line 2)) (8.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/site-packages (from matplotlib~=3.0->gluonts==0.6.7->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/site-packages (from matplotlib~=3.0->gluonts==0.6.7->-r requirements.txt (line 2)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/site-packages (from matplotlib~=3.0->gluonts==0.6.7->-r requirements.txt (line 2)) (2.4.7)\u001b[0m\n",
      "\u001b[34mCollecting dataclasses>=0.6; python_version < \"3.7\"\n",
      "  Downloading https://files.pythonhosted.org/packages/fe/ca/75fac5856ab5cfa51bbbcefa250182e50441074fdc3f803f6e76451fab43/dataclasses-0.8-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mCollecting pymeeus<=1,>=0.3.13\n",
      "  Downloading https://files.pythonhosted.org/packages/c7/ff/0f0a0becf088281c6bc6c75b7d7c03a2481d486ef6cc7c8899bbcab0a88d/PyMeeus-0.5.11.tar.gz (5.4MB)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pandas, korean-lunar-calendar, pymeeus, convertdate, hijri-converter, holidays, toolz, ujson, dataclasses, pydantic, gluonts\n",
      "  Found existing installation: pandas 0.25.1\n",
      "    Uninstalling pandas-0.25.1:\n",
      "      Successfully uninstalled pandas-0.25.1\u001b[0m\n",
      "\u001b[34m    Running setup.py install for pymeeus: started\n",
      "    Running setup.py install for pymeeus: finished with status 'done'\n",
      "    Running setup.py install for ujson: started\u001b[0m\n",
      "\u001b[34m    Running setup.py install for ujson: finished with status 'done'\u001b[0m\n",
      "\u001b[34mSuccessfully installed convertdate-2.3.2 dataclasses-0.8 gluonts-0.6.7 hijri-converter-2.1.1 holidays-0.11.1 korean-lunar-calendar-0.2.1 pandas-1.1.5 pydantic-1.6.1 pymeeus-0.5.11 toolz-0.11.1 ujson-1.35\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 19.3.1; however, version 21.0.1 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2021-04-07 06:59:20,038 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-04-07 06:59:20,051 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-04-07 06:59:20,063 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-04-07 06:59:20,073 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_mxnet_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"lr\": 0.001,\n",
      "        \"epochs\": 30\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"mxnet-training-2021-04-07-06-55-54-757\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-143656149352/mxnet-training-2021-04-07-06-55-54-757/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":30,\"lr\":0.001}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_mxnet_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-143656149352/mxnet-training-2021-04-07-06-55-54-757/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":30,\"lr\":0.001},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"mxnet-training-2021-04-07-06-55-54-757\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-143656149352/mxnet-training-2021-04-07-06-55-54-757/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"30\",\"--lr\",\"0.001\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.001\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=30\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python36.zip:/usr/local/lib/python3.6:/usr/local/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.6 train.py --epochs 30 --lr 0.001\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mlearning rate from ``lr_scheduler`` has been overwritten by ``learning_rate`` in optimizer.\u001b[0m\n",
      "\u001b[34m[2021-04-07 06:59:23.761 ip-10-2-126-145.ec2.internal:53 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-04-07 06:59:23.761 ip-10-2-126-145.ec2.internal:53 INFO hook.py:193] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-04-07 06:59:23.761 ip-10-2-126-145.ec2.internal:53 INFO hook.py:238] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-04-07 06:59:23.761 ip-10-2-126-145.ec2.internal:53 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-04-07 06:59:23.787 ip-10-2-126-145.ec2.internal:53 INFO hook.py:398] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2021-04-07 06:59:23.787 ip-10-2-126-145.ec2.internal:53 INFO hook.py:461] Hook is writing from the hook with pid: 53\n",
      "\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/50 [00:00<?, ?it/s]INFO:gluonts.trainer:Number of parameters in DeepARTrainingNetwork: 16419\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 50/50 [00:02<00:00, 23.46it/s, epoch=1/30, avg_epoch_loss=2.44]\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[0] Elapsed time 2.133 seconds\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[0] Evaluation metric 'epoch_loss'=2.438762\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[1] Learning rate is 0.001\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/50 [00:00<?, ?it/s]#015100%|██████████| 50/50 [00:01<00:00, 28.97it/s, epoch=2/30, avg_epoch_loss=1.53]\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[1] Elapsed time 1.726 seconds\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[1] Evaluation metric 'epoch_loss'=1.532745\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[2] Learning rate is 0.001\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/50 [00:00<?, ?it/s]#015100%|██████████| 50/50 [00:01<00:00, 27.94it/s, epoch=3/30, avg_epoch_loss=1.38]\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[2] Elapsed time 1.790 seconds\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[2] Evaluation metric 'epoch_loss'=1.380245\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[3] Learning rate is 0.001\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/50 [00:00<?, ?it/s]#015100%|██████████| 50/50 [00:01<00:00, 26.86it/s, epoch=4/30, avg_epoch_loss=1.31]\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[3] Elapsed time 1.862 seconds\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[3] Evaluation metric 'epoch_loss'=1.310146\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[4] Learning rate is 0.001\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/50 [00:00<?, ?it/s]#015100%|██████████| 50/50 [00:01<00:00, 27.62it/s, epoch=5/30, avg_epoch_loss=1.29]\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[4] Elapsed time 1.811 seconds\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[4] Evaluation metric 'epoch_loss'=1.291735\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[5] Learning rate is 0.001\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/50 [00:00<?, ?it/s]#015100%|██████████| 50/50 [00:01<00:00, 28.14it/s, epoch=6/30, avg_epoch_loss=1.29]\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[5] Elapsed time 1.777 seconds\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[5] Evaluation metric 'epoch_loss'=1.288782\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[6] Learning rate is 0.001\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/50 [00:00<?, ?it/s]#015100%|██████████| 50/50 [00:01<00:00, 28.68it/s, epoch=7/30, avg_epoch_loss=1.27]\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[6] Elapsed time 1.744 seconds\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[6] Evaluation metric 'epoch_loss'=1.273217\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[7] Learning rate is 0.001\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/50 [00:00<?, ?it/s]#015100%|██████████| 50/50 [00:01<00:00, 28.18it/s, epoch=8/30, avg_epoch_loss=1.22]\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[7] Elapsed time 1.775 seconds\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[7] Evaluation metric 'epoch_loss'=1.222292\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[8] Learning rate is 0.001\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/50 [00:00<?, ?it/s]#015100%|██████████| 50/50 [00:01<00:00, 28.99it/s, epoch=9/30, avg_epoch_loss=1.17]\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[8] Elapsed time 1.725 seconds\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[8] Evaluation metric 'epoch_loss'=1.165298\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[9] Learning rate is 0.001\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/50 [00:00<?, ?it/s]#015100%|██████████| 50/50 [00:01<00:00, 27.72it/s, epoch=10/30, avg_epoch_loss=1.16]\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[9] Elapsed time 1.804 seconds\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[9] Evaluation metric 'epoch_loss'=1.155793\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[10] Learning rate is 0.001\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/50 [00:00<?, ?it/s]#015100%|██████████| 50/50 [00:01<00:00, 28.82it/s, epoch=11/30, avg_epoch_loss=1.19]\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[10] Elapsed time 1.735 seconds\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[10] Evaluation metric 'epoch_loss'=1.186078\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[11] Learning rate is 0.001\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/50 [00:00<?, ?it/s]#015100%|██████████| 50/50 [00:01<00:00, 26.17it/s, epoch=12/30, avg_epoch_loss=1.12]\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[11] Elapsed time 1.911 seconds\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[11] Evaluation metric 'epoch_loss'=1.117182\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[12] Learning rate is 0.001\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/50 [00:00<?, ?it/s]#015100%|██████████| 50/50 [00:01<00:00, 27.36it/s, epoch=13/30, avg_epoch_loss=1.1]\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[12] Elapsed time 1.828 seconds\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[12] Evaluation metric 'epoch_loss'=1.095529\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[13] Learning rate is 0.001\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/50 [00:00<?, ?it/s]#015100%|██████████| 50/50 [00:01<00:00, 29.06it/s, epoch=14/30, avg_epoch_loss=1.09]\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[13] Elapsed time 1.721 seconds\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[13] Evaluation metric 'epoch_loss'=1.086271\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[14] Learning rate is 0.001\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/50 [00:00<?, ?it/s]#015100%|██████████| 50/50 [00:01<00:00, 26.67it/s, epoch=15/30, avg_epoch_loss=1.06]\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[14] Elapsed time 1.875 seconds\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[14] Evaluation metric 'epoch_loss'=1.059926\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[15] Learning rate is 0.001\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/50 [00:00<?, ?it/s]#015100%|██████████| 50/50 [00:01<00:00, 27.08it/s, epoch=16/30, avg_epoch_loss=1.04]\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[15] Elapsed time 1.847 seconds\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[15] Evaluation metric 'epoch_loss'=1.041950\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[16] Learning rate is 0.001\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/50 [00:00<?, ?it/s]#015100%|██████████| 50/50 [00:01<00:00, 28.64it/s, epoch=17/30, avg_epoch_loss=1.08]\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[16] Elapsed time 1.746 seconds\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[16] Evaluation metric 'epoch_loss'=1.079451\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[17] Learning rate is 0.001\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/50 [00:00<?, ?it/s]#015100%|██████████| 50/50 [00:01<00:00, 29.25it/s, epoch=18/30, avg_epoch_loss=1.01]\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[17] Elapsed time 1.710 seconds\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[17] Evaluation metric 'epoch_loss'=1.011432\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[18] Learning rate is 0.001\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/50 [00:00<?, ?it/s]#015100%|██████████| 50/50 [00:01<00:00, 27.92it/s, epoch=19/30, avg_epoch_loss=0.991]\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[18] Elapsed time 1.791 seconds\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[18] Evaluation metric 'epoch_loss'=0.991275\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[19] Learning rate is 0.001\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/50 [00:00<?, ?it/s]#015100%|██████████| 50/50 [00:01<00:00, 29.13it/s, epoch=20/30, avg_epoch_loss=0.987]\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[19] Elapsed time 1.717 seconds\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[19] Evaluation metric 'epoch_loss'=0.986705\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[20] Learning rate is 0.001\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/50 [00:00<?, ?it/s]#015100%|██████████| 50/50 [00:01<00:00, 26.39it/s, epoch=21/30, avg_epoch_loss=0.995]\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[20] Elapsed time 1.895 seconds\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[20] Evaluation metric 'epoch_loss'=0.995340\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[21] Learning rate is 0.001\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/50 [00:00<?, ?it/s]#015100%|██████████| 50/50 [00:02<00:00, 24.97it/s, epoch=22/30, avg_epoch_loss=1]\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[21] Elapsed time 2.003 seconds\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[21] Evaluation metric 'epoch_loss'=1.001767\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[22] Learning rate is 0.001\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m#015  0%|          | 0/50 [00:00<?, ?it/s]#015100%|██████████| 50/50 [00:02<00:00, 22.65it/s, epoch=23/30, avg_epoch_loss=0.991]\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[22] Elapsed time 2.209 seconds\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[22] Evaluation metric 'epoch_loss'=0.990915\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[23] Learning rate is 0.001\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/50 [00:00<?, ?it/s]#015100%|██████████| 50/50 [00:02<00:00, 25.00it/s, epoch=24/30, avg_epoch_loss=0.959]\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[23] Elapsed time 2.001 seconds\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[23] Evaluation metric 'epoch_loss'=0.958770\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[24] Learning rate is 0.001\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/50 [00:00<?, ?it/s]#015100%|██████████| 50/50 [00:01<00:00, 28.01it/s, epoch=25/30, avg_epoch_loss=0.962]\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[24] Elapsed time 1.785 seconds\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[24] Evaluation metric 'epoch_loss'=0.962210\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[25] Learning rate is 0.001\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/50 [00:00<?, ?it/s]#015100%|██████████| 50/50 [00:01<00:00, 28.15it/s, epoch=26/30, avg_epoch_loss=0.92]\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[25] Elapsed time 1.777 seconds\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[25] Evaluation metric 'epoch_loss'=0.920179\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[26] Learning rate is 0.001\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/50 [00:00<?, ?it/s]#015100%|██████████| 50/50 [00:01<00:00, 27.30it/s, epoch=27/30, avg_epoch_loss=0.945]\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[26] Elapsed time 1.832 seconds\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[26] Evaluation metric 'epoch_loss'=0.945473\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[27] Learning rate is 0.001\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/50 [00:00<?, ?it/s]#015100%|██████████| 50/50 [00:01<00:00, 26.59it/s, epoch=28/30, avg_epoch_loss=0.945]\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[27] Elapsed time 1.881 seconds\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[27] Evaluation metric 'epoch_loss'=0.944574\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[28] Learning rate is 0.001\u001b[0m\n",
      "\n",
      "2021-04-07 07:00:22 Uploading - Uploading generated training model\u001b[34m#015  0%|          | 0/50 [00:00<?, ?it/s]#015100%|██████████| 50/50 [00:01<00:00, 27.76it/s, epoch=29/30, avg_epoch_loss=0.905]\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[28] Elapsed time 1.802 seconds\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[28] Evaluation metric 'epoch_loss'=0.904583\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[29] Learning rate is 0.001\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/50 [00:00<?, ?it/s]#015100%|██████████| 50/50 [00:01<00:00, 27.47it/s, epoch=30/30, avg_epoch_loss=0.912]\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[29] Elapsed time 1.820 seconds\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:Epoch[29] Evaluation metric 'epoch_loss'=0.911937\u001b[0m\n",
      "\u001b[34mINFO:root:Computing averaged parameters.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading averaged parameters.\u001b[0m\n",
      "\u001b[34mINFO:gluonts.trainer:End model training\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/mxnet/gluon/block.py:1111: UserWarning: \"gluonts.model.deepar._network.DeepARTrainingNetwork(alpha=0.0, beta=0.0, cardinality=[3], cell_type=\"lstm\", context_length=12, distr_output=gluonts.mx.distribution.student_t.StudentTOutput(), dropout_rate=0.1, dropoutcell_type=\"ZoneoutCell\", dtype=numpy.float32, embedding_dimension=[2], history_length=1105, lags_seq=[1, 2, 3, 4, 5, 6, 7, 8, 13, 14, 15, 20, 21, 22, 27, 28, 29, 30, 31, 56, 84, 363, 364, 365, 727, 728, 729, 1091, 1092, 1093], num_cells=30, num_layers=2, prediction_length=12, scaling=True)\" is being hybridized while still having forward hook/pre-hook. If \"gluonts.model.deepar._network.DeepARTrainingNetwork(alpha=0.0, beta=0.0, cardinality=[3], cell_type=\"lstm\", context_length=12, distr_output=gluonts.mx.distribution.student_t.StudentTOutput(), dropout_rate=0.1, dropoutcell_type=\"ZoneoutCell\", dtype=numpy.float32, embedding_dimension=[2], history_length=1105, lags_seq=[1, 2, 3, 4, 5, 6, 7, 8, 13, 14, 15, 20, 21, 22, 27, 28, 29, 30, 31, 56, 84, 363, 364, 365, 727, 728, 729, 1091, 1092, 1093], num_cells=30, num_layers=2, prediction_length=12, scaling=True)\" is a child of HybridBlock, the hooks will not take effect.\n",
      "  .format(block=self))\u001b[0m\n",
      "\u001b[34m#015Running evaluation:   0%|          | 0/45 [00:00<?, ?it/s]#015Running evaluation: 100%|██████████| 45/45 [00:00<00:00, 2099.30it/s]\u001b[0m\n",
      "\u001b[34m{\n",
      "  \"MAPE\": 0.08456071348047647,\n",
      "  \"RMSE\": 1.245190815750878,\n",
      "  \"wQuantileLoss[0.1]\": 0.04383334392787284,\n",
      "  \"wQuantileLoss[0.5]\": 0.08881945813990273,\n",
      "  \"wQuantileLoss[0.9]\": 0.03600422305325051,\n",
      "  \"mean_wQuantileLoss\": 0.05621900837367536\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mWARNING:root:Serializing RepresentableBlockPredictor instances does not save the prediction network structure in a backwards-compatible manner. Be careful not to use this method in production.\u001b[0m\n",
      "\u001b[34m2021-04-07 07:00:19,903 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-04-07 07:00:40 Completed - Training job completed\n",
      "Training seconds: 132\n",
      "Billable seconds: 132\n"
     ]
    }
   ],
   "source": [
    "s3_input = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}'.format(bucket, prefix))\n",
    "estimator.fit(s3_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Getting Model Artifacts\n",
    "---\n",
    "\n",
    "훈련이 완료된 모델 아티팩트를 로컬(노트북 인스턴스 or 온프레미스)로 복사합니다. 훈련 완료 시 `SM_MODEL_DIR`에 있는 파일들이\n",
    "`model.tar.gz`로 자동으로 압축되며, 압축을 해제하여 로컬 상에서도 추론을 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_dir = './model'\n",
    "!rm -rf $local_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-143656149352/mxnet-training-2021-04-07-06-55-54-757/output/\n",
      "2021-04-07 07:00:25      62118 model.tar.gz\n",
      "download: s3://sagemaker-us-east-1-143656149352/mxnet-training-2021-04-07-06-55-54-757/output/model.tar.gz to model/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import json , os\n",
    "\n",
    "s3_model_dir = estimator.model_data.replace('model.tar.gz', '')\n",
    "print(s3_model_dir)\n",
    "!aws s3 ls {s3_model_dir}\n",
    "\n",
    "if not os.path.exists(local_model_dir):\n",
    "    os.makedirs(local_model_dir)\n",
    "\n",
    "!aws s3 cp {s3_model_dir}model.tar.gz {local_model_dir}/model.tar.gz\n",
    "!tar -xzf {local_model_dir}/model.tar.gz -C {local_model_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 모듈에서 활용할 변수들을 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 's3_model_dir' (str)\n",
      "Stored 'prefix' (str)\n"
     ]
    }
   ],
   "source": [
    "%store s3_model_dir\n",
    "%store prefix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
