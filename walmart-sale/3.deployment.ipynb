{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3. Deployment on MMS(Multi Model Server)\n",
    "---\n",
    "\n",
    "본 모듈에서는 모델의 배포(deployment)를 수행합니다. 노트북 실행에는 약 15분 가량 소요되며, 핸즈온 실습 시에는 25분을 권장드립니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "## 1. Inference script\n",
    "---\n",
    "\n",
    "아래 코드 셀은 `src` 디렉토리에 SageMaker 추론 스크립트인 `inference.py`를 저장합니다.<br>\n",
    "\n",
    "이 스크립트는 SageMaker 상에서 MMS(Multi Model Server)를 쉽고 편하게 배포할 수 이는 high-level 툴킷인 SageMaker inference toolkit의 인터페이스를\n",
    "사용하고 있으며, 여러분께서는 인터페이스에 정의된 핸들러(handler) 함수들만 구현하시면 됩니다.\n",
    "\n",
    "#### MMS(Multi Model Server)란?\n",
    "- [https://github.com/awslabs/multi-model-server](https://github.com/awslabs/multi-model-server) (2017년 12월 초 MXNet 1.0 릴리스 시 최초 공개, MXNet용 모델 서버로 시작)\n",
    "- Prerequisites: Java 8, MXNet (단, MXNet 사용 시에만)\n",
    "- MMS는 프레임워크에 구애받지 않도록 설계되었기 때문에, 모든 프레임워크의 백엔드 엔진 역할을 할 수 있는 충분한 유연성을 제공합니다.\n",
    "- SageMaker MXNet 추론 컨테이너와 PyTorch 추론 컨테이너는 SageMaker inference toolkit으로 MMS를 래핑하여 사용합니다.\n",
    "    - 2020년 4월 말 PyTorch용 배포 웹 서비스인 torchserve가 출시되면서, 향후 PyTorch 추론 컨테이너는 MMS 기반에서 torchserve 기반으로 마이그레이션될 예정입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/inference.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import gluonts \n",
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import pathlib\n",
    "from mxnet import gpu, cpu\n",
    "from mxnet.context import num_gpus\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gluonts.dataset.util import to_pandas\n",
    "from gluonts.mx.distribution import DistributionOutput, StudentTOutput, NegativeBinomialOutput, GaussianOutput\n",
    "from gluonts.model.deepar import DeepAREstimator\n",
    "from gluonts.mx.trainer import Trainer\n",
    "from gluonts.evaluation import Evaluator\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions, backtest_metrics\n",
    "from gluonts.model.predictor import Predictor\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.dataset.common import ListDataset\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    path = pathlib.Path(model_dir)   \n",
    "    predictor = Predictor.deserialize(path)\n",
    "    print(\"model was loaded successfully\")\n",
    "    return predictor\n",
    "\n",
    "\n",
    "def transform_fn(model, request_body, content_type='application/json', accept_type='application/json'):\n",
    "\n",
    "    related_cols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment'] \n",
    "    item_cols = ['Type', 'Size'] \n",
    "    FREQ = 'W'\n",
    "    pred_length = 12\n",
    "\n",
    "    data = json.loads(request_body)    \n",
    "    \n",
    "    target_test_df = pd.DataFrame(data['target_values'], index=data['timestamp'])\n",
    "    related_test_df = pd.DataFrame(data['related_values'], index=data['timestamp'])\n",
    "    item_df = pd.DataFrame(data['item'], index=data['store_id'])\n",
    "    item_df.columns = item_cols\n",
    "        \n",
    "    target = target_test_df.values\n",
    "    num_steps, num_series = target_test_df.shape\n",
    "    start_dt = target_test_df.index[0]\n",
    "    \n",
    "    num_related_cols = len(related_cols)\n",
    "    num_features_per_feature = int(related_test_df.shape[1] / num_related_cols)\n",
    "    related_list = []\n",
    "\n",
    "    for feature_idx in range(0, num_related_cols):\n",
    "        start_idx = feature_idx * num_features_per_feature\n",
    "        end_idx = start_idx + num_features_per_feature\n",
    "        related_list.append(related_test_df.iloc[:, start_idx:end_idx].values)\n",
    "\n",
    "    test_lst = []\n",
    "    for i in range(0, num_series):\n",
    "        target_vec = target[:-pred_length, i]\n",
    "        related_vecs = [related[:, i] for related in related_list]\n",
    "        item = item_df.loc[i+1]    \n",
    "        dic = {FieldName.TARGET: target_vec, \n",
    "               FieldName.START: start_dt,\n",
    "               FieldName.FEAT_DYNAMIC_REAL: related_vecs,\n",
    "               FieldName.FEAT_STATIC_CAT: [item[0]],\n",
    "               FieldName.FEAT_STATIC_REAL: [item[1]]\n",
    "              } \n",
    "        test_lst.append(dic)\n",
    "\n",
    "    test_ds = ListDataset(test_lst, freq=FREQ)\n",
    "\n",
    "    response_body = {}\n",
    "    forecast_it = model.predict(test_ds)\n",
    "\n",
    "    for idx, f in enumerate(forecast_it):\n",
    "        response_body[f'store_{idx}'] = f.samples.mean(axis=0).tolist()\n",
    "\n",
    "    return json.dumps(response_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Test Inference code \n",
    "---\n",
    "\n",
    "엔드포인트 배포 전, 추론 스크립트를 검증합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model was loaded successfully\n",
      "[16.235092163085938, 16.1125545501709, 16.108495712280273, 16.64717674255371, 16.730663299560547, 16.33971405029297, 16.174470901489258, 17.156557083129883, 17.097583770751953, 16.813976287841797, 16.475114822387695, 18.01433563232422]\n"
     ]
    }
   ],
   "source": [
    "from src.inference import model_fn, transform_fn\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare test data \n",
    "target_test_df = pd.read_csv(\"data/target_train.csv\", index_col=0, header=[0,1])\n",
    "related_test_df = pd.read_csv(\"data/related_train.csv\", index_col=0, header=[0,1])\n",
    "item_df = pd.read_csv(\"data/item.csv\", index_col=0)\n",
    "\n",
    "input_data = {'target_values': target_test_df.values.tolist(), \n",
    "              'related_values': related_test_df.values.tolist(),\n",
    "              'item': item_df.values.tolist(),\n",
    "              'timestamp': related_test_df.index.tolist(),\n",
    "              'store_id': item_df.index.tolist()\n",
    "             }\n",
    "\n",
    "request_body = json.dumps(input_data)\n",
    "# Test inference script \n",
    "model = model_fn('./model')\n",
    "response = transform_fn(model, request_body)\n",
    "outputs = json.loads(response)\n",
    "print(outputs['store_0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Local Endpoint Inference\n",
    "---\n",
    "\n",
    "충분한 검증 및 테스트 없이 훈련된 모델을 곧바로 실제 운영 환경에 배포하기에는 많은 위험 요소들이 있습니다. 따라서, 로컬 모드를 사용하여 실제 운영 환경에 배포하기 위한 추론 인스턴스를 시작하기 전에 노트북 인스턴스의 로컬 환경에서 모델을 배포하는 것을 권장합니다. 이를 로컬 모드 엔드포인트(Local Mode Endpoint)라고 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sagemaker\n",
    "from sagemaker.mxnet import MXNetModel\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_path = f'file://{os.getcwd()}/model/model.tar.gz'\n",
    "endpoint_name = \"local-endpoint-walmart-sale-forecast-{}\".format(int(time.time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "아래 코드 셀을 실행 후, 로그를 확인해 보세요. MMS에 대한 세팅값들을 확인하실 수 있습니다.\n",
    "\n",
    "```bash\n",
    "algo-1-u3xwd_1  | MMS Home: /usr/local/lib/python3.6/site-packages\n",
    "algo-1-u3xwd_1  | Current directory: /\n",
    "algo-1-u3xwd_1  | Temp directory: /home/model-server/tmp\n",
    "algo-1-u3xwd_1  | Number of GPUs: 0\n",
    "algo-1-u3xwd_1  | Number of CPUs: 2\n",
    "algo-1-u3xwd_1  | Max heap size: 878 M\n",
    "algo-1-u3xwd_1  | Python executable: /usr/local/bin/python3.6\n",
    "algo-1-u3xwd_1  | Config file: /etc/sagemaker-mms.properties\n",
    "algo-1-u3xwd_1  | Inference address: http://0.0.0.0:8080\n",
    "algo-1-u3xwd_1  | Management address: http://0.0.0.0:8080\n",
    "algo-1-u3xwd_1  | Model Store: /.sagemaker/mms/models\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to 3j551y0yug-algo-1-df9jt\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Collecting pandas==1.1.5\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m   Downloading pandas-1.1.5-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.5 MB 35.2 MB/s eta 0:00:01\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m \u001b[?25hCollecting gluonts==0.7.6\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m   Downloading gluonts-0.7.6-py3-none-any.whl (897 kB)\n",
      "\u001b[K     |████████████████████████████████| 897 kB 54.3 MB/s eta 0:00:01\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m \u001b[?25hCollecting pydantic~=1.1\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m   Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.1 MB 56.4 MB/s eta 0:00:01\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m \u001b[?25hRequirement already satisfied: tqdm~=4.23 in /usr/local/lib/python3.7/site-packages (from gluonts==0.7.6->-r /opt/ml/model/code/requirements.txt (line 2)) (4.60.0)\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.7/site-packages (from gluonts==0.7.6->-r /opt/ml/model/code/requirements.txt (line 2)) (3.4.1)\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Requirement already satisfied: numpy~=1.16 in /usr/local/lib/python3.7/site-packages (from gluonts==0.7.6->-r /opt/ml/model/code/requirements.txt (line 2)) (1.17.4)\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Collecting holidays>=0.9\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m   Downloading holidays-0.11.1-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 68.8 MB/s eta 0:00:01\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m \u001b[?25hCollecting toolz~=0.10\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m   Downloading toolz-0.11.1-py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 8.2 MB/s  eta 0:00:01\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m \u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/site-packages (from pandas==1.1.5->-r /opt/ml/model/code/requirements.txt (line 1)) (2.8.1)\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Collecting pytz>=2017.2\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m   Downloading pytz-2021.1-py2.py3-none-any.whl (510 kB)\n",
      "\u001b[K     |████████████████████████████████| 510 kB 63.7 MB/s eta 0:00:01\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m \u001b[?25hCollecting korean-lunar-calendar\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m   Downloading korean_lunar_calendar-0.2.1-py3-none-any.whl (8.0 kB)\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Collecting convertdate>=2.3.0\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m   Downloading convertdate-2.3.2-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 9.3 MB/s  eta 0:00:01\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m \u001b[?25hCollecting hijri-converter\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m   Downloading hijri_converter-2.1.3-py3-none-any.whl (14 kB)\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from holidays>=0.9->gluonts==0.7.6->-r /opt/ml/model/code/requirements.txt (line 2)) (1.15.0)\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Collecting pymeeus<=1,>=0.3.13\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m   Downloading PyMeeus-0.5.11.tar.gz (5.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.4 MB 68.3 MB/s eta 0:00:01\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m \u001b[?25hRequirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/site-packages (from matplotlib~=3.0->gluonts==0.7.6->-r /opt/ml/model/code/requirements.txt (line 2)) (2.4.7)\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/site-packages (from matplotlib~=3.0->gluonts==0.7.6->-r /opt/ml/model/code/requirements.txt (line 2)) (8.2.0)\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/site-packages (from matplotlib~=3.0->gluonts==0.7.6->-r /opt/ml/model/code/requirements.txt (line 2)) (1.3.1)\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/site-packages (from matplotlib~=3.0->gluonts==0.7.6->-r /opt/ml/model/code/requirements.txt (line 2)) (0.10.0)\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/site-packages (from pydantic~=1.1->gluonts==0.7.6->-r /opt/ml/model/code/requirements.txt (line 2)) (3.7.4.3)\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Using legacy 'setup.py install' for pymeeus, since package 'wheel' is not installed.\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Installing collected packages: pytz, pymeeus, korean-lunar-calendar, hijri-converter, convertdate, toolz, pydantic, pandas, holidays, gluonts\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m     Running setup.py install for pymeeus ... \u001b[?25ldone\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m \u001b[?25hSuccessfully installed convertdate-2.3.2 gluonts-0.7.6 hijri-converter-2.1.3 holidays-0.11.1 korean-lunar-calendar-0.2.1 pandas-1.1.5 pydantic-1.8.2 pymeeus-0.5.11 pytz-2021.1 toolz-0.11.1\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m \u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.3 is available.\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Warning: Calling MMS with mxnet-model-server. Please move to multi-model-server.\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:01,983 [INFO ] main com.amazonaws.ml.mms.ModelServer - \n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m MMS Home: /usr/local/lib/python3.7/site-packages\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Current directory: /\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Temp directory: /home/model-server/tmp\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Number of GPUs: 0\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Number of CPUs: 4\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Max heap size: 1696 M\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Python executable: /usr/local/bin/python3.7\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Config file: /etc/sagemaker-mms.properties\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Inference address: http://0.0.0.0:8080\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Management address: http://0.0.0.0:8080\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Model Store: /.sagemaker/mms/models\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Initial Models: ALL\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Log dir: /logs\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Metrics dir: /logs\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Netty threads: 0\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Netty client threads: 0\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Default workers per model: 4\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Blacklist Regex: N/A\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Maximum Response Size: 6553500\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Maximum Request Size: 6553500\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Preload model: false\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Prefer direct buffer: false\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:02,063 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-9000-model\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:02,127 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - model_service_worker started with args: --sock-type unix --sock-name /home/model-server/tmp/.mms.sock.9000 --handler sagemaker_mxnet_serving_container.handler_service --model-path /.sagemaker/mms/models/model --model-name model --preload-model false --tmp-dir /home/model-server/tmp\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:02,128 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9000\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:02,128 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID] 60\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:02,128 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MMS worker started.\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:02,129 [INFO ] main com.amazonaws.ml.mms.wlm.ModelManager - Model model loaded.\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:02,130 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.7.10\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:02,137 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:02,145 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:02,145 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:02,145 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:02,146 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:02,198 [INFO ] main com.amazonaws.ml.mms.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m Model server started.\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:02,210 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:02,210 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:02,211 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:02,212 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:02,218 [WARN ] pool-2-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - worker pid is not available yet.\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:02,223 [ERROR] pool-2-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - \n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m java.io.IOException: Stream closed\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m \tat java.lang.ProcessBuilder$NullOutputStream.write(ProcessBuilder.java:433)\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m \tat java.io.OutputStream.write(OutputStream.java:116)\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m \tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m \tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m \tat java.io.FilterOutputStream.close(FilterOutputStream.java:158)\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m \tat com.amazonaws.ml.mms.metrics.MetricCollector.run(MetricCollector.java:76)\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m \tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m \tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m \tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m \tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m \tat java.lang.Thread.run(Thread.java:748)\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:04,639 [INFO ] pool-1-thread-6 ACCESS_LOG - /172.19.0.1:53570 \"GET /ping HTTP/1.1\" 200 42\n",
      "!\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:05,707 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - generated new fontManager\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:05,753 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - generated new fontManager\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:05,777 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - generated new fontManager\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:05,805 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - generated new fontManager\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:06,045 [WARN ] W-9000-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /usr/local/lib/python3.7/site-packages/gluonts/json.py:46: UserWarning: Using `json`-module for json-handling. Consider installing one of `orjson`, `ujson` to speed up serialization and deserialization.\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:06,045 [WARN ] W-9000-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   \"Using `json`-module for json-handling. \"\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:06,095 [WARN ] W-9000-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /usr/local/lib/python3.7/site-packages/gluonts/json.py:46: UserWarning: Using `json`-module for json-handling. Consider installing one of `orjson`, `ujson` to speed up serialization and deserialization.\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:06,095 [WARN ] W-9000-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   \"Using `json`-module for json-handling. \"\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:06,121 [WARN ] W-9000-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /usr/local/lib/python3.7/site-packages/gluonts/json.py:46: UserWarning: Using `json`-module for json-handling. Consider installing one of `orjson`, `ujson` to speed up serialization and deserialization.\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:06,122 [WARN ] W-9000-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   \"Using `json`-module for json-handling. \"\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:06,145 [WARN ] W-9000-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /usr/local/lib/python3.7/site-packages/gluonts/json.py:46: UserWarning: Using `json`-module for json-handling. Consider installing one of `orjson`, `ujson` to speed up serialization and deserialization.\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:06,145 [WARN ] W-9000-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   \"Using `json`-module for json-handling. \"\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:06,198 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Using CPU\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:06,224 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - model was loaded successfully\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:06,224 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242acfffe130002-0000002b-00000004-2a4ebc5a799c1fe7-0d0d3de3\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:06,226 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3952\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:06,230 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-1\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:06,239 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Using CPU\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:06,253 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - model was loaded successfully\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:06,254 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3984\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:06,255 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-3\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:06,255 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242acfffe130002-0000002b-00000000-78d9dc5a799c1fe7-aef3a668\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:06,272 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Using CPU\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:06,288 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Using CPU\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:06,294 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - model was loaded successfully\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:06,294 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 4012\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:06,294 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242acfffe130002-0000002b-00000001-1b15dc5a799c1fe7-8d1a3568\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:06,294 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-2\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:06,304 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - model was loaded successfully\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:06,305 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242acfffe130002-0000002b-00000002-12743c5a799c1fe7-7d9ef075\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:06,305 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 4035\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:06,305 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-4\n"
     ]
    }
   ],
   "source": [
    "local_model = MXNetModel(model_data=local_model_path,\n",
    "                         role=role,\n",
    "                         source_dir='src',\n",
    "                         entry_point='inference.py',\n",
    "                         framework_version='1.8.0',\n",
    "                         py_version='py37')\n",
    "\n",
    "predictor = local_model.deploy(instance_type='local', \n",
    "                           initial_instance_count=1, \n",
    "                           endpoint_name=endpoint_name,\n",
    "                           wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로컬에서 컨테이너를 배포했기 때문에 컨테이너가 현재 실행 중임을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE                                                                         COMMAND                  CREATED             STATUS              PORTS                              NAMES\n",
      "797dbd73418e        763104351884.dkr.ecr.us-east-1.amazonaws.com/mxnet-inference:1.8.0-cpu-py37   \"python /usr/local/b…\"   24 seconds ago      Up 20 seconds       0.0.0.0:8080->8080/tcp, 8081/tcp   3j551y0yug-algo-1-df9jt\n"
     ]
    }
   ],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference using SageMaker SDK\n",
    "\n",
    "SageMaker SDK의 `predict()` 메서드로 쉽게 추론을 수행할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:18,213 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 986\r\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:18,214 [INFO ] W-9000-model ACCESS_LOG - /172.19.0.1:53626 \"POST /invocations HTTP/1.1\" 200 991\r\n",
      "[16.23509407043457, 16.1125545501709, 16.108495712280273, 16.64717674255371, 16.730663299560547, 16.33971405029297, 16.174468994140625, 17.156557083129883, 17.097583770751953, 16.81397819519043, 16.475114822387695, 18.01433563232422] [7.053330898284912, 7.185001373291016, 7.335565090179443, 7.1755452156066895, 7.255197048187256, 7.280794620513916, 7.130575656890869, 7.46973991394043, 7.510270595550537, 7.223666191101074, 7.417709827423096, 7.8838300704956055]\n"
     ]
    }
   ],
   "source": [
    "outputs = predictor.predict(input_data)\n",
    "print(outputs['store_0'], outputs['store_20'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference using Boto3 SDK\n",
    "\n",
    "SageMaker SDK의 `predict()` 메서드로 추론을 수행할 수도 있지만, 이번에는 boto3의 `invoke_endpoint()` 메서드로 추론을 수행해 보겠습니다.<br>\n",
    "Boto3는 서비스 레벨의 low-level SDK로, ML 실험에 초점을 맞춰 일부 기능들이 추상화된 high-level SDK인 SageMaker SDK와 달리\n",
    "SageMaker API를 완벽하게 제어할 수 있습으며, 프로덕션 및 자동화 작업에 적합합니다.\n",
    "\n",
    "참고로 `invoke_endpoint()` 호출을 위한 런타임 클라이언트 인스턴스 생성 시, 로컬 배포 모드에서는 `sagemaker.local.LocalSagemakerRuntimeClient()`를 호출해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:24,867 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 972\r\n",
      "\u001b[36m3j551y0yug-algo-1-df9jt |\u001b[0m 2021-06-30 14:12:24,867 [INFO ] W-9000-model ACCESS_LOG - /172.19.0.1:53630 \"POST /invocations HTTP/1.1\" 200 974\r\n"
     ]
    }
   ],
   "source": [
    "client = sagemaker.local.LocalSagemakerClient()\n",
    "runtime_client = sagemaker.local.LocalSagemakerRuntimeClient()\n",
    "endpoint_name = local_model.endpoint_name\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType='application/json',\n",
    "    Accept='application/json',\n",
    "    Body=json.dumps(input_data)\n",
    "    )\n",
    "outputs = json.loads(response['Body'].read().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.23509407043457, 16.1125545501709, 16.108495712280273, 16.64717674255371, 16.730663299560547, 16.33971405029297, 16.174468994140625, 17.156557083129883, 17.097583770751953, 16.81397819519043, 16.475114822387695, 18.01433563232422] [7.053330898284912, 7.185001373291016, 7.335565090179443, 7.1755452156066895, 7.255197048187256, 7.280794620513916, 7.130575656890869, 7.46973991394043, 7.510270595550537, 7.223666191101074, 7.417709827423096, 7.8838300704956055]\n"
     ]
    }
   ],
   "source": [
    "print(outputs['store_0'], outputs['store_20'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Mode Endpoint Clean-up\n",
    "\n",
    "엔드포인트를 계속 사용하지 않는다면, 엔드포인트를 삭제해야 합니다. \n",
    "SageMaker SDK에서는 `delete_endpoint()` 메소드로 간단히 삭제할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gracefully stopping... (press Ctrl+C again to force)\n",
      "--- Deleted model: mxnet-inference-2021-06-30-14-11-19-730\n",
      "--- Deleted endpoint: local-endpoint-walmart-sale-forecast-1625062271\n",
      "--- Deleted endpoint_config: local-endpoint-walmart-sale-forecast-1625062271\n"
     ]
    }
   ],
   "source": [
    "def delete_endpoint(client, endpoint_name):\n",
    "    response = client.describe_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "    model_name = response['ProductionVariants'][0]['ModelName']\n",
    "\n",
    "    client.delete_model(ModelName=model_name)    \n",
    "    client.delete_endpoint(EndpointName=endpoint_name)\n",
    "    client.delete_endpoint_config(EndpointConfigName=endpoint_name)    \n",
    "    \n",
    "    print(f'--- Deleted model: {model_name}')\n",
    "    print(f'--- Deleted endpoint: {endpoint_name}')\n",
    "    print(f'--- Deleted endpoint_config: {endpoint_name}')    \n",
    "delete_endpoint(client, endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. SageMaker Hosted Endpoint Inference\n",
    "---\n",
    "\n",
    "이제 실제 운영 환경에 엔드포인트 배포를 수행해 보겠습니다. 로컬 모드 엔드포인트와 대부분의 코드가 동일하며, 모델 아티팩트 경로(`model_data`)와 인스턴스 유형(`instance_type`)만 변경해 주시면 됩니다. SageMaker가 관리하는 배포 클러스터를 프로비저닝하는 시간이 소요되기 때문에 추론 서비스를 시작하는 데에는 약 5~10분 정도 소요됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.mxnet import MXNet\n",
    "\n",
    "boto_session = boto3.Session()\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto_session)\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker.Session().default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(s3_model_dir, \"model.tar.gz\")\n",
    "endpoint_name = \"endpoint-walmart-sale-forecast-{}\".format(int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "model = MXNetModel(model_data=model_path,\n",
    "                         role=role,\n",
    "                         source_dir='src',\n",
    "                         entry_point='inference.py',\n",
    "                         framework_version='1.8.0',\n",
    "                         py_version='py37')\n",
    "\n",
    "predictor = model.deploy(instance_type=\"ml.c5.large\", \n",
    "                         initial_instance_count=1, \n",
    "                         endpoint_name=endpoint_name,\n",
    "                         wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추론을 수행합니다. 로컬 모드의 코드와 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "client = boto3.client('sagemaker')\n",
    "runtime_client = boto3.client('sagemaker-runtime')\n",
    "endpoint_name = model.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType='application/json',\n",
    "    Accept='application/json',\n",
    "    Body=json.dumps(input_data)\n",
    "    )\n",
    "outputs = json.loads(response['Body'].read().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.23509407043457, 16.1125545501709, 16.108495712280273, 16.64717674255371, 16.730663299560547, 16.33971405029297, 16.174468994140625, 17.156557083129883, 17.097583770751953, 16.81397819519043, 16.475114822387695, 18.01433563232422] [7.053330898284912, 7.185001373291016, 7.335565090179443, 7.1755452156066895, 7.255197048187256, 7.280794620513916, 7.130575656890869, 7.46973991394043, 7.510270595550537, 7.223666191101074, 7.417709827423096, 7.8838300704956055]\n"
     ]
    }
   ],
   "source": [
    "print(outputs['store_0'], outputs['store_20'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Hosted Endpoint Clean-up\n",
    "\n",
    "엔드포인트를 계속 사용하지 않는다면, 불필요한 과금을 피하기 위해 엔드포인트를 삭제해야 합니다. \n",
    "SageMaker SDK에서는 `delete_endpoint()` 메소드로 간단히 삭제할 수 있으며, UI에서도 쉽게 삭제할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Deleted model: mxnet-inference-2021-06-30-14-12-57-286\n",
      "--- Deleted endpoint: endpoint-walmart-sale-forecast-1625062360\n",
      "--- Deleted endpoint_config: endpoint-walmart-sale-forecast-1625062360\n"
     ]
    }
   ],
   "source": [
    "delete_endpoint(client, endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
